"""
ResNet128 æ·±åº¦å­¦ä¹ è®­ç»ƒè„šæœ¬
ä½¿ç”¨ Weights & Biases (wandb) è¿›è¡Œå®éªŒè·Ÿè¸ªã€å¯è§†åŒ–å’Œæ¨¡å‹ç®¡ç†

"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import wandb
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
from datetime import datetime

# ============================================================================
# 1. å®éªŒé…ç½®å’Œwandbåˆå§‹åŒ–
# ============================================================================

def init_wandb():
    """
    åˆå§‹åŒ–wandbå®éªŒè·Ÿè¸ª
    è¿™é‡Œä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å®éªŒè¿è¡Œï¼Œå¹¶è®¾ç½®é¡¹ç›®åç§°
    """
    # è®¾ç½®é¡¹ç›®åç§°å’Œå®éªŒåç§°
    project_name = "resnet128-training"
    experiment_name = f"resnet128-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    
    # åˆå§‹åŒ–wandb
    wandb.init(
        project=project_name,
        name=experiment_name,
        config={
            # æ¨¡å‹è¶…å‚æ•°
            "model": "ResNet128",
            "num_classes": 10,
            "learning_rate": 0.001,
            "batch_size": 32,
            "epochs": 100,
            "optimizer": "Adam",
            "scheduler": "StepLR",
            "step_size": 30,
            "gamma": 0.1,
            
            # æ•°æ®å¢å¼ºå‚æ•°
            "data_augmentation": True,
            "random_crop": 32,
            "random_horizontal_flip": 0.5,
            "normalize_mean": [0.4914, 0.4822, 0.4465],
            "normalize_std": [0.2023, 0.1994, 0.2010],
            
            # è®­ç»ƒå‚æ•°
            "early_stopping_patience": 10,
            "save_best_model": True,
            "model_save_path": "./models"
        }
    )
    
    print(f"âœ… Wandbå®éªŒå·²åˆå§‹åŒ–: {experiment_name}")
    return wandb.config

# ============================================================================
# 2. ResNet128 æ¨¡å‹å®šä¹‰
# ============================================================================

class BasicBlock(nn.Module):
    """
    ResNetçš„åŸºæœ¬æ®‹å·®å—
    åŒ…å«ä¸¤ä¸ª3x3å·ç§¯å±‚å’Œè·³è·ƒè¿æ¥
    """
    def __init__(self, in_channels, out_channels, stride=1):
        super(BasicBlock, self).__init__()
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å±‚
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # ç¬¬äºŒä¸ªå·ç§¯å±‚
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # è·³è·ƒè¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, 
                          stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        # ä¸»è·¯å¾„
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        # è·³è·ƒè¿æ¥
        out += self.shortcut(x)
        out = torch.relu(out)
        
        return out

class ResNet128(nn.Module):
    """
    ResNet128 æ¨¡å‹
    åŒ…å«å¤šä¸ªæ®‹å·®å±‚ï¼Œç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡
    """
    def __init__(self, num_classes=10):
        super(ResNet128, self).__init__()
        
        # åˆå§‹å·ç§¯å±‚
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        
        # æ®‹å·®å±‚ç»„
        # æ¯ç»„åŒ…å«å¤šä¸ªæ®‹å·®å—ï¼Œé€šé“æ•°é€æ¸å¢åŠ 
        self.layer1 = self._make_layer(64, 64, 2, stride=1)      # 64 -> 64
        self.layer2 = self._make_layer(64, 128, 2, stride=2)     # 64 -> 128
        self.layer3 = self._make_layer(128, 256, 2, stride=2)    # 128 -> 256
        self.layer4 = self._make_layer(256, 512, 2, stride=2)    # 256 -> 512
        
        # å…¨å±€å¹³å‡æ± åŒ–å’Œåˆ†ç±»å™¨
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
        
        # æƒé‡åˆå§‹åŒ–
        self._initialize_weights()
    
    def _make_layer(self, in_channels, out_channels, num_blocks, stride):
        """
        åˆ›å»ºåŒ…å«å¤šä¸ªæ®‹å·®å—çš„å±‚
        """
        layers = []
        # ç¬¬ä¸€ä¸ªå—å¯èƒ½éœ€è¦æ”¹å˜é€šé“æ•°å’Œæ­¥é•¿
        layers.append(BasicBlock(in_channels, out_channels, stride))
        
        # å…¶ä½™å—ä¿æŒç›¸åŒçš„é€šé“æ•°å’Œæ­¥é•¿
        for _ in range(1, num_blocks):
            layers.append(BasicBlock(out_channels, out_channels, 1))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        """
        åˆå§‹åŒ–æ¨¡å‹æƒé‡
        ä½¿ç”¨Heåˆå§‹åŒ–æ–¹æ³•ï¼Œé€‚åˆReLUæ¿€æ´»å‡½æ•°
        """
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # åˆå§‹å·ç§¯
        x = torch.relu(self.bn1(self.conv1(x)))
        
        # æ®‹å·®å±‚
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        
        # åˆ†ç±»
        x = self.fc(x)
        
        return x

# ============================================================================
# 3. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
# ============================================================================

def get_data_loaders(config):
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
    åŒ…å«æ•°æ®å¢å¼ºå’Œæ ‡å‡†åŒ–
    """
    print("ğŸ“Š æ­£åœ¨å‡†å¤‡æ•°æ®åŠ è½½å™¨...")
    
    # æ•°æ®å¢å¼ºå˜æ¢
    if config.data_augmentation:
        train_transform = transforms.Compose([
            transforms.RandomCrop(config.random_crop),
            transforms.RandomHorizontalFlip(config.random_horizontal_flip),
            transforms.ToTensor(),
            transforms.Normalize(config.normalize_mean, config.normalize_std)
        ])
    else:
        train_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(config.normalize_mean, config.normalize_std)
        ])
    
    # éªŒè¯é›†å˜æ¢ï¼ˆæ— æ•°æ®å¢å¼ºï¼‰
    val_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(config.normalize_mean, config.normalize_std)
    ])
    
    # åŠ è½½CIFAR-10æ•°æ®é›†
    # CIFAR-10åŒ…å«10ä¸ªç±»åˆ«çš„32x32å½©è‰²å›¾åƒ
    train_dataset = torchvision.datasets.CIFAR10(
        root='./data', 
        train=True, 
        download=True, 
        transform=train_transform
    )
    
    val_dataset = torchvision.datasets.CIFAR10(
        root='./data', 
        train=False, 
        download=True, 
        transform=val_transform
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.batch_size, 
        shuffle=True, 
        num_workers=2,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config.batch_size, 
        shuffle=False, 
        num_workers=2,
        pin_memory=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
    print(f"   - è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   - éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"   - æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    
    return train_loader, val_loader

# ============================================================================
# 4. è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
# ============================================================================

def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    è¿”å›è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1} [è®­ç»ƒ]')
    
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    avg_loss = running_loss / len(train_loader)
    avg_acc = 100. * correct / total
    
    return avg_loss, avg_acc

def validate_epoch(model, val_loader, criterion, device):
    """
    éªŒè¯ä¸€ä¸ªepoch
    è¿”å›éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
    """
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
        pbar = tqdm(val_loader, desc='[éªŒè¯]')
        
        for data, target in pbar:
            data, target = data.to(device), target.to(device)
            
            # å‰å‘ä¼ æ’­
            output = model(data)
            loss = criterion(output, target)
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    avg_loss = running_loss / len(val_loader)
    avg_acc = 100. * correct / total
    
    return avg_loss, avg_acc

# ============================================================================
# 5. ä¸»è®­ç»ƒå¾ªç¯
# ============================================================================

def main():
    """
    ä¸»è®­ç»ƒå‡½æ•°
    åŒ…å«å®Œæ•´çš„è®­ç»ƒæµç¨‹å’Œwandbé›†æˆ
    """
    print("ğŸš€ å¼€å§‹ResNet128è®­ç»ƒæµç¨‹...")
    
    # 1. åˆå§‹åŒ–wandb
    config = init_wandb()
    
    # 2. è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 3. åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸ æ­£åœ¨åˆ›å»ºResNet128æ¨¡å‹...")
    model = ResNet128(num_classes=config.num_classes).to(device)
    
    # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
    print(f"   - æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = get_data_loaders(config)
    
    # 5. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)
    
    # 6. åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    os.makedirs(config.model_save_path, exist_ok=True)
    
    # 7. è®­ç»ƒå¾ªç¯
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå…± {config.epochs} ä¸ªepoch...")
    
    best_val_acc = 0.0
    patience_counter = 0
    
    # è®°å½•è®­ç»ƒå†å²
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []
    
    for epoch in range(config.epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{config.epochs}")
        print(f"{'='*60}")
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch
        )
        
        # éªŒè¯
        val_loss, val_acc = validate_epoch(
            model, val_loader, criterion, device
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # è®°å½•åˆ°wandb
        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "train_accuracy": train_acc,
            "val_loss": val_loss,
            "val_accuracy": val_acc,
            "learning_rate": current_lr
        })
        
        # ä¿å­˜è®­ç»ƒå†å²
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        
        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch+1} ç»“æœ:")
        print(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"   éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        print(f"   å­¦ä¹ ç‡: {current_lr:.6f}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if config.save_best_model:
                best_model_path = os.path.join(config.model_save_path, "best_model.pth")
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_val_acc': best_val_acc,
                    'config': dict(config)
                }, best_model_path)
                
                print(f"ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path}")
                
                # å°†æ¨¡å‹ä¸Šä¼ åˆ°wandb
                model_artifact = wandb.Artifact(
                    name=f"best-model-{wandb.run.id}",
                    type="model",
                    description="ResNet128æœ€ä½³éªŒè¯å‡†ç¡®ç‡æ¨¡å‹"
                )
                model_artifact.add_file(best_model_path)
                wandb.log_artifact(model_artifact)
                print("âœ… æœ€ä½³æ¨¡å‹å·²ä¸Šä¼ åˆ°wandb")
        else:
            patience_counter += 1
            print(f"â³ éªŒè¯å‡†ç¡®ç‡æœªæå‡ï¼Œè€å¿ƒè®¡æ•°: {patience_counter}/{config.early_stopping_patience}")
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= config.early_stopping_patience:
            print(f"ğŸ›‘ æ—©åœè§¦å‘ï¼éªŒè¯å‡†ç¡®ç‡è¿ç»­ {config.early_stopping_patience} ä¸ªepochæœªæå‡")
            break
    
    # 8. è®­ç»ƒå®Œæˆ
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = os.path.join(config.model_save_path, "final_model.pth")
    torch.save({
        'epoch': epoch + 1,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'final_val_acc': val_acc,
        'best_val_acc': best_val_acc,
        'config': dict(config)
    }, final_model_path)
    
    print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # 9. åˆ›å»ºè®­ç»ƒæ›²çº¿å›¾
    create_training_plots(train_losses, train_accs, val_losses, val_accs)
    
    # 10. å®Œæˆwandbè¿è¡Œ
    wandb.finish()
    print("âœ… Wandbè¿è¡Œå·²å®Œæˆ")

def create_training_plots(train_losses, train_accs, val_losses, val_accs):
    """
    åˆ›å»ºè®­ç»ƒæ›²çº¿å›¾å¹¶ä¸Šä¼ åˆ°wandb
    """
    print("ğŸ“ˆ æ­£åœ¨åˆ›å»ºè®­ç»ƒæ›²çº¿å›¾...")
    
    # åˆ›å»ºå›¾è¡¨
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±æ›²çº¿
    epochs = range(1, len(train_losses) + 1)
    ax1.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±')
    ax1.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±')
    ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('æŸå¤±')
    ax1.legend()
    ax1.grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    ax2.plot(epochs, train_accs, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')
    ax2.plot(epochs, val_accs, 'r-', label='éªŒè¯å‡†ç¡®ç‡')
    ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('å‡†ç¡®ç‡ (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = "training_curves.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # ä¸Šä¼ åˆ°wandb
    wandb.log({"training_curves": wandb.Image(plot_path)})
    print(f"âœ… è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜å¹¶ä¸Šä¼ åˆ°wandb: {plot_path}")

# ============================================================================
# 6. è„šæœ¬å…¥å£
# ============================================================================

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        wandb.finish()
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        wandb.finish()
        raise e 